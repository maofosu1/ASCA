# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VOy6uQxjE-eMXmJ8YKIFZ-USwBpV4SFP
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols, mixedlm
from statsmodels.stats.anova import anova_lm
import random
from scipy.linalg import svd
import warnings
warnings.filterwarnings('ignore')

# Load the synthetic data
df = pd.read_csv('synthetic_multivariate_data.csv')

# Extract feature columns
feature_cols = [col for col in df.columns if 'feature' in col]
X = df[feature_cols].values

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create a data matrix for easier handling
X_matrix = pd.DataFrame(X_scaled, columns=feature_cols)
X_matrix['subject_id'] = df['subject_id']
X_matrix['treatment'] = df['treatment']
X_matrix['timepoint'] = df['timepoint']

print("Data prepared for analysis.")
print("Number of samples:", X_matrix.shape[0])
print("Number of features:", len(feature_cols))
print("Factors: treatment, timepoint, subject_id")

# ============ ASCA: ANOVA-Simultaneous Component Analysis ============

def asca(data, formula, design_cols, feature_cols):
    """
    Implement ASCA (ANOVA-Simultaneous Component Analysis)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    formula: str
        ANOVA formula for decomposition
    design_cols: list
        List of column names for design factors
    feature_cols: list
        List of column names for features

    Returns:
    --------
    Dictionary containing:
    - effect_matrices: matrices for each effect
    - pca_results: PCA results for each effect
    - explained_var: explained variance for each effect
    - total_variance: total variance of the data
    """
    print("\n=== ASCA Analysis ===")

    # Extract design and data matrices
    X = data[feature_cols].values
    n_samples, n_features = X.shape

    # Center the data
    X_centered = X - np.mean(X, axis=0)

    # Fit the ANOVA model for each feature
    effect_matrices = {}
    anova_summaries = []

    # ANOVA decomposition
    for feature_idx, feature in enumerate(feature_cols):
        # Create formula for this feature
        feature_formula = f"{feature} ~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"

        # Fit ANOVA model
        model = ols(feature_formula, data=data).fit()
        anova_table = sm.stats.anova_lm(model, typ=2)

        # Store ANOVA summary for this feature
        if feature_idx == 0:  # Only store once
            anova_summaries.append(anova_table)

    # Extract effects from ANOVA model
    # Create empty matrices for each effect
    effects = ['C(treatment)', 'C(timepoint)', 'C(treatment):C(timepoint)', 'Residual']
    effect_matrices = {effect: np.zeros((n_samples, n_features)) for effect in effects}

    # Fill effect matrices feature by feature
    for feature_idx, feature in enumerate(feature_cols):
        # Create formula for this feature
        feature_formula = f"{feature} ~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"

        # Fit ANOVA model for this feature
        model = ols(feature_formula, data=data).fit()

        # Get predicted values for each effect
        # Treatment effect
        treatment_effect = np.zeros(n_samples)
        for idx, row in data.iterrows():
            treatment = row['treatment']
            param_name = f"C(treatment)[T.{treatment}]"
            if param_name in model.params:
                treatment_effect[idx] = model.params[param_name]
        effect_matrices['C(treatment)'][:, feature_idx] = treatment_effect

        # Timepoint effect
        timepoint_effect = np.zeros(n_samples)
        for idx, row in data.iterrows():
            timepoint = row['timepoint']
            param_name = f"C(timepoint)[T.{timepoint}]"
            if param_name in model.params:
                timepoint_effect[idx] = model.params[param_name]
        effect_matrices['C(timepoint)'][:, feature_idx] = timepoint_effect

        # Interaction effect
        interaction_effect = np.zeros(n_samples)
        for idx, row in data.iterrows():
            treatment = row['treatment']
            timepoint = row['timepoint']
            param_name = f"C(treatment)[T.{treatment}]:C(timepoint)[T.{timepoint}]"
            if param_name in model.params:
                interaction_effect[idx] = model.params[param_name]
        effect_matrices['C(treatment):C(timepoint)'][:, feature_idx] = interaction_effect

        # Residual effect
        effect_matrices['Residual'][:, feature_idx] = model.resid

    # Apply PCA to each effect matrix
    pca_results = {}
    explained_var = {}

    for effect, matrix in effect_matrices.items():
        if effect != 'Residual':  # Skip PCA on residuals
            pca = PCA()
            pca_results[effect] = pca.fit_transform(matrix)
            explained_var[effect] = pca.explained_variance_ratio_

            # Print effect summary
            effect_ss = np.sum(matrix**2)
            print(f"{effect} - Sum of Squares: {effect_ss:.4f}")
            if len(explained_var[effect]) > 0:
                print(f"  PC1 explained variance: {explained_var[effect][0]*100:.2f}%")
                if len(explained_var[effect]) > 1:
                    print(f"  PC2 explained variance: {explained_var[effect][1]*100:.2f}%")

    # Calculate total variance
    total_variance = np.sum(X_centered**2)

    return {
        'effect_matrices': effect_matrices,
        'pca_results': pca_results,
        'explained_var': explained_var,
        'total_variance': total_variance,
        'anova_summaries': anova_summaries
    }

# ============ ASCA+ (ASCA with Significance Testing) ============

def asca_plus(data, formula, design_cols, feature_cols, n_permutations=999):
    """
    Implement ASCA+ (ASCA with permutation testing)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    formula: str
        ANOVA formula for decomposition
    design_cols: list
        List of column names for design factors
    feature_cols: list
        List of column names for features
    n_permutations: int
        Number of permutations for significance testing

    Returns:
    --------
    Dictionary containing ASCA results plus permutation test results
    """
    print("\n=== ASCA+ Analysis (ASCA with Permutation Testing) ===")

    # Run standard ASCA
    asca_results = asca(data, formula, design_cols, feature_cols)

    # Get effect matrices and calculate sum of squares for each effect
    effect_matrices = asca_results['effect_matrices']
    observed_ss = {effect: np.sum(matrix**2) for effect, matrix in effect_matrices.items()
                 if effect != 'Residual'}

    # Permutation testing
    permuted_ss = {effect: [] for effect in observed_ss.keys()}

    print(f"\nRunning permutation tests with {n_permutations} permutations...")

    for perm in range(n_permutations):
        if perm % 100 == 0 and perm > 0:
            print(f"  Completed {perm} permutations")

        # Create permuted data
        perm_data = data.copy()

        # Permute each factor separately
        for effect in observed_ss.keys():
            if effect == 'C(treatment)':
                # Permute treatment labels
                perm_data['treatment'] = np.random.permutation(perm_data['treatment'].values)
            elif effect == 'C(timepoint)':
                # Permute timepoint labels
                perm_data['timepoint'] = np.random.permutation(perm_data['timepoint'].values)
            elif effect == 'C(treatment):C(timepoint)':
                # Permute interaction by shuffling combinations
                combinations = perm_data[['treatment', 'timepoint']].values
                np.random.shuffle(combinations)
                perm_data['treatment'] = combinations[:, 0]
                perm_data['timepoint'] = combinations[:, 1]

        # Run ASCA on permuted data
        perm_asca = asca(perm_data, formula, design_cols, feature_cols)
        perm_effect_matrices = perm_asca['effect_matrices']

        # Calculate sum of squares for each permuted effect
        for effect in observed_ss.keys():
            perm_ss = np.sum(perm_effect_matrices[effect]**2)
            permuted_ss[effect].append(perm_ss)

    # Calculate p-values
    p_values = {}
    for effect in observed_ss.keys():
        # Count how many permuted SS are >= observed SS
        p_values[effect] = (np.sum(np.array(permuted_ss[effect]) >= observed_ss[effect]) + 1) / (n_permutations + 1)

    print("\nPermutation Test Results:")
    for effect, p_value in p_values.items():
        significance = "significant" if p_value < 0.05 else "not significant"
        print(f"{effect}: p-value = {p_value:.4f} ({significance})")

    # Add results to ASCA results
    asca_results['p_values'] = p_values
    asca_results['permuted_ss'] = permuted_ss

    return asca_results

# ============ PE-ASCA (Permutation-Enhanced ASCA) ============

def pe_asca(data, formula, design_cols, feature_cols, n_permutations=999, alpha=0.05):
    """
    Implement PE-ASCA (Permutation-Enhanced ASCA)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    formula: str
        ANOVA formula for decomposition
    design_cols: list
        List of column names for design factors
    feature_cols: list
        List of column names for features
    n_permutations: int
        Number of permutations for significance testing
    alpha: float
        Significance level

    Returns:
    --------
    Dictionary containing PE-ASCA results
    """
    print("\n=== PE-ASCA Analysis (Permutation-Enhanced ASCA) ===")

    # First, run ASCA+ to get permutation test results
    asca_plus_results = asca_plus(data, formula, design_cols, feature_cols, n_permutations)

    # Extract relevant information
    effect_matrices = asca_plus_results['effect_matrices']
    p_values = asca_plus_results['p_values']

    # Enhanced decomposition: only keep significant effects
    enhanced_matrices = {}
    for effect, p_value in p_values.items():
        if p_value <= alpha:
            enhanced_matrices[effect] = effect_matrices[effect]
            print(f"Effect {effect} is significant (p={p_value:.4f}) and included in enhanced model")
        else:
            # Add non-significant effects to residuals
            if 'Residual' not in enhanced_matrices:
                enhanced_matrices['Residual'] = effect_matrices[effect].copy()
            else:
                enhanced_matrices['Residual'] += effect_matrices[effect]
            print(f"Effect {effect} is not significant (p={p_value:.4f}) and added to residuals")

    # Add original residuals if not already included
    if 'Residual' not in enhanced_matrices:
        enhanced_matrices['Residual'] = effect_matrices['Residual']
    else:
        enhanced_matrices['Residual'] += effect_matrices['Residual']

    # Apply PCA to each significant effect matrix
    pca_results = {}
    explained_var = {}

    for effect, matrix in enhanced_matrices.items():
        if effect != 'Residual':  # Skip PCA on residuals
            pca = PCA()
            pca_results[effect] = pca.fit_transform(matrix)
            explained_var[effect] = pca.explained_variance_ratio_

            # Print effect summary
            effect_ss = np.sum(matrix**2)
            total_ss = np.sum([np.sum(m**2) for m in enhanced_matrices.values()])
            print(f"{effect} - Sum of Squares: {effect_ss:.4f} ({effect_ss/total_ss*100:.2f}% of total)")
            if len(explained_var[effect]) > 0:
                print(f"  PC1 explained variance: {explained_var[effect][0]*100:.2f}%")
                if len(explained_var[effect]) > 1:
                    print(f"  PC2 explained variance: {explained_var[effect][1]*100:.2f}%")

    # Create result structure
    pe_asca_results = {
        'effect_matrices': enhanced_matrices,
        'pca_results': pca_results,
        'explained_var': explained_var,
        'p_values': p_values,
        'total_variance': asca_plus_results['total_variance']
    }

    return pe_asca_results

# ============ LiMM-PCA (Linear Mixed Models with PCA) ============

def limm_pca(data, feature_cols, verbose=True):
    """
    Implement LiMM-PCA (Linear Mixed Models with PCA)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    feature_cols: list
        List of column names for features
    verbose: bool
        Whether to print detailed output

    Returns:
    --------
    Dictionary containing LiMM-PCA results
    """
    if verbose:
        print("\n=== LiMM-PCA Analysis (Linear Mixed Models with PCA) ===")

    # Extract data matrix
    X = data[feature_cols].values

    # Standardize the data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Initialize containers for results
    fixed_effects = {}
    random_effects = {}
    residuals = {}
    pvalues = {}

    # Apply mixed model to each feature
    for i, feature in enumerate(feature_cols):
        if verbose and i % 3 == 0:
            print(f"Processing feature {i+1}/{len(feature_cols)}...")

        # Create mixed model formula
        formula = f"{feature} ~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"

        # Fit mixed effects model
        try:
            model = mixedlm(formula, data, groups=data['subject_id'])
            result = model.fit(reml=True)

            # Extract fixed effects
            fe_design = pd.get_dummies(data[['treatment', 'timepoint']], drop_first=False)
            fixed_pred = result.predict(exog=fe_design)
            fixed_effects[feature] = fixed_pred

            # Extract random effects (subject)
            rand_pred = result.random_effects
            subject_effect = np.zeros(len(data))
            for subject, effect in rand_pred.items():
                subject_idx = data['subject_id'] == subject
                subject_effect[subject_idx] = effect[0]
            random_effects[feature] = subject_effect

            # Extract residuals
            residuals[feature] = result.resid

            # Store p-values for fixed effects
            pvalues[feature] = result.pvalues

        except Exception as e:
            if verbose:
                print(f"Error fitting model for {feature}: {e}")
            # Use simpler approach if model fails
            fixed_effects[feature] = np.zeros(len(data))
            random_effects[feature] = np.zeros(len(data))
            residuals[feature] = X_scaled[:, i]

    # Convert to matrices
    fixed_matrix = np.column_stack([fixed_effects[feature] for feature in feature_cols])
    random_matrix = np.column_stack([random_effects[feature] for feature in feature_cols])
    resid_matrix = np.column_stack([residuals[feature] for feature in feature_cols])

    # Calculate variance explained
    total_var = np.sum(X_scaled**2)
    fixed_var = np.sum(fixed_matrix**2)
    random_var = np.sum(random_matrix**2)
    resid_var = np.sum(resid_matrix**2)

    if verbose:
        print("\nVariance Partition:")
        print(f"Fixed Effects: {fixed_var/total_var*100:.2f}%")
        print(f"Random Effects: {random_var/total_var*100:.2f}%")
        print(f"Residuals: {resid_var/total_var*100:.2f}%")

    # Apply PCA to fixed effects matrix
    pca_fixed = PCA()
    pca_fixed.fit(fixed_matrix)

    if verbose:
        print("\nPCA on Fixed Effects:")
        print(f"PC1 explained variance: {pca_fixed.explained_variance_ratio_[0]*100:.2f}%")
        if len(pca_fixed.explained_variance_ratio_) > 1:
            print(f"PC2 explained variance: {pca_fixed.explained_variance_ratio_[1]*100:.2f}%")

    # Store results
    results = {
        'fixed_matrix': fixed_matrix,
        'random_matrix': random_matrix,
        'resid_matrix': resid_matrix,
        'pca_fixed': pca_fixed,
        'fixed_scores': pca_fixed.transform(fixed_matrix),
        'total_var': total_var,
        'fixed_var': fixed_var,
        'random_var': random_var,
        'resid_var': resid_var
    }

    return results

# ============ MSCA (Multilevel Simultaneous Component Analysis) ============

def msca(data, feature_cols, verbose=True):
    """
    Implement MSCA (Multilevel Simultaneous Component Analysis)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    feature_cols: list
        List of column names for features
    verbose: bool
        Whether to print detailed output

    Returns:
    --------
    Dictionary containing MSCA results
    """
    if verbose:
        print("\n=== MSCA Analysis (Multilevel Simultaneous Component Analysis) ===")

    # Extract data
    X = data[feature_cols].values
    subjects = data['subject_id'].unique()
    n_subjects = len(subjects)

    # Center the data
    X_centered = X - np.mean(X, axis=0)

    # Split data into between-subject and within-subject parts
    X_between = np.zeros_like(X)
    X_within = np.zeros_like(X)

    # Calculate between-subject variation (level 2)
    for subject in subjects:
        subject_indices = data['subject_id'] == subject
        subject_mean = np.mean(X[subject_indices], axis=0)
        X_between[subject_indices] = subject_mean

    # Calculate within-subject variation (level 1)
    X_within = X - X_between

    # Calculate variance explained by each level
    total_var = np.sum(X_centered**2)
    between_var = np.sum(X_between**2)
    within_var = np.sum(X_within**2)

    if verbose:
        print("\nVariance Partition:")
        print(f"Between-subject: {between_var/total_var*100:.2f}%")
        print(f"Within-subject: {within_var/total_var*100:.2f}%")

    # Apply PCA to between-subject variation
    pca_between = PCA()
    between_scores = pca_between.fit_transform(X_between)

    # Apply PCA to within-subject variation
    pca_within = PCA()
    within_scores = pca_within.fit_transform(X_within)

    if verbose:
        print("\nPCA on Between-subject Variation:")
        print(f"PC1 explained variance: {pca_between.explained_variance_ratio_[0]*100:.2f}%")
        if len(pca_between.explained_variance_ratio_) > 1:
            print(f"PC2 explained variance: {pca_between.explained_variance_ratio_[1]*100:.2f}%")

        print("\nPCA on Within-subject Variation:")
        print(f"PC1 explained variance: {pca_within.explained_variance_ratio_[0]*100:.2f}%")
        if len(pca_within.explained_variance_ratio_) > 1:
            print(f"PC2 explained variance: {pca_within.explained_variance_ratio_[1]*100:.2f}%")

    # Store results
    results = {
        'X_between': X_between,
        'X_within': X_within,
        'pca_between': pca_between,
        'pca_within': pca_within,
        'between_scores': between_scores,
        'within_scores': within_scores,
        'total_var': total_var,
        'between_var': between_var,
        'within_var': within_var
    }

    return results

# Run all methods on the synthetic data

print("\n==== RUNNING ALL METHODS ON THE SYNTHETIC DATASET ====")

# Prepare formula and design columns for ASCA methods
formula = "~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"
design_cols = ['treatment', 'timepoint', 'subject_id']

# Run each method
print("\n===== METHOD 1: ASCA =====")
asca_results = asca(X_matrix, formula, design_cols, feature_cols)

print("\n===== METHOD 2: ASCA+ =====")
asca_plus_results = asca_plus(X_matrix, formula, design_cols, feature_cols, n_permutations=99)  # Reduced for computation

print("\n===== METHOD 3: PE-ASCA =====")
pe_asca_results = pe_asca(X_matrix, formula, design_cols, feature_cols, n_permutations=99, alpha=0.05)  # Reduced for computation

print("\n===== METHOD 4: LiMM-PCA =====")
limm_pca_results = limm_pca(X_matrix, feature_cols)

print("\n===== METHOD 5: MSCA =====")
msca_results = msca(X_matrix, feature_cols)

"""New

"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols, mixedlm
from statsmodels.stats.anova import anova_lm
import random
from scipy.linalg import svd
import warnings
warnings.filterwarnings('ignore')

# Load the synthetic data
df = pd.read_csv('synthetic_multivariate_data.csv')

# Extract feature columns
feature_cols = [col for col in df.columns if 'feature' in col]
X = df[feature_cols].values

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create a data matrix for easier handling
X_matrix = pd.DataFrame(X_scaled, columns=feature_cols)
X_matrix['subject_id'] = df['subject_id']
X_matrix['treatment'] = df['treatment']
X_matrix['timepoint'] = df['timepoint']

print("Data prepared for analysis.")
print("Number of samples:", X_matrix.shape[0])
print("Number of features:", len(feature_cols))
print("Factors: treatment, timepoint, subject_id")

# ============ ASCA: ANOVA-Simultaneous Component Analysis ============

def asca(data, formula, design_cols, feature_cols):
    """
    Implement ASCA (ANOVA-Simultaneous Component Analysis)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    formula: str
        ANOVA formula for decomposition
    design_cols: list
        List of column names for design factors
    feature_cols: list
        List of column names for features

    Returns:
    --------
    Dictionary containing:
    - effect_matrices: matrices for each effect
    - pca_results: PCA results for each effect
    - explained_var: explained variance for each effect
    - total_variance: total variance of the data
    """
    print("\n=== ASCA Analysis ===")

    # Extract design and data matrices
    X = data[feature_cols].values
    n_samples, n_features = X.shape

    # Center the data
    X_centered = X - np.mean(X, axis=0)

    # Fit the ANOVA model for each feature
    effect_matrices = {}
    anova_summaries = []

    # ANOVA decomposition
    for feature_idx, feature in enumerate(feature_cols):
        # Create formula for this feature
        feature_formula = f"{feature} ~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"

        # Fit ANOVA model
        model = ols(feature_formula, data=data).fit()
        anova_table = sm.stats.anova_lm(model, typ=2)

        # Store ANOVA summary for this feature
        if feature_idx == 0:  # Only store once
            anova_summaries.append(anova_table)

    # Extract effects from ANOVA model
    # Create empty matrices for each effect
    effects = ['C(treatment)', 'C(timepoint)', 'C(treatment):C(timepoint)', 'Residual']
    effect_matrices = {effect: np.zeros((n_samples, n_features)) for effect in effects}

    # Fill effect matrices feature by feature
    for feature_idx, feature in enumerate(feature_cols):
        # Create formula for this feature
        feature_formula = f"{feature} ~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"

        # Fit ANOVA model for this feature
        model = ols(feature_formula, data=data).fit()

        # Get predicted values for each effect
        # Treatment effect
        treatment_effect = np.zeros(n_samples)
        for idx, row in data.iterrows():
            treatment = row['treatment']
            param_name = f"C(treatment)[T.{treatment}]"
            if param_name in model.params:
                treatment_effect[idx] = model.params[param_name]
        effect_matrices['C(treatment)'][:, feature_idx] = treatment_effect

        # Timepoint effect
        timepoint_effect = np.zeros(n_samples)
        for idx, row in data.iterrows():
            timepoint = row['timepoint']
            param_name = f"C(timepoint)[T.{timepoint}]"
            if param_name in model.params:
                timepoint_effect[idx] = model.params[param_name]
        effect_matrices['C(timepoint)'][:, feature_idx] = timepoint_effect

        # Interaction effect
        interaction_effect = np.zeros(n_samples)
        for idx, row in data.iterrows():
            treatment = row['treatment']
            timepoint = row['timepoint']
            param_name = f"C(treatment)[T.{treatment}]:C(timepoint)[T.{timepoint}]"
            if param_name in model.params:
                interaction_effect[idx] = model.params[param_name]
        effect_matrices['C(treatment):C(timepoint)'][:, feature_idx] = interaction_effect

        # Residual effect
        effect_matrices['Residual'][:, feature_idx] = model.resid

    # Apply PCA to each effect matrix
    pca_results = {}
    explained_var = {}

    for effect, matrix in effect_matrices.items():
        if effect != 'Residual':  # Skip PCA on residuals
            pca = PCA()
            pca_results[effect] = pca.fit_transform(matrix)
            explained_var[effect] = pca.explained_variance_ratio_

            # Print effect summary
            effect_ss = np.sum(matrix**2)
            print(f"{effect} - Sum of Squares: {effect_ss:.4f}")
            if len(explained_var[effect]) > 0:
                print(f"  PC1 explained variance: {explained_var[effect][0]*100:.2f}%")
                if len(explained_var[effect]) > 1:
                    print(f"  PC2 explained variance: {explained_var[effect][1]*100:.2f}%")

    # Calculate total variance
    total_variance = np.sum(X_centered**2)

    return {
        'effect_matrices': effect_matrices,
        'pca_results': pca_results,
        'explained_var': explained_var,
        'total_variance': total_variance,
        'anova_summaries': anova_summaries
    }

# ============ ASCA+ (ASCA with Significance Testing) ============

def asca_plus(data, formula, design_cols, feature_cols, n_permutations=999):
    """
    Implement ASCA+ (ASCA with permutation testing)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    formula: str
        ANOVA formula for decomposition
    design_cols: list
        List of column names for design factors
    feature_cols: list
        List of column names for features
    n_permutations: int
        Number of permutations for significance testing

    Returns:
    --------
    Dictionary containing ASCA results plus permutation test results
    """
    print("\n=== ASCA+ Analysis (ASCA with Permutation Testing) ===")

    # Run standard ASCA
    asca_results = asca(data, formula, design_cols, feature_cols)

    # Get effect matrices and calculate sum of squares for each effect
    effect_matrices = asca_results['effect_matrices']
    observed_ss = {effect: np.sum(matrix**2) for effect, matrix in effect_matrices.items()
                 if effect != 'Residual'}

    # Permutation testing
    permuted_ss = {effect: [] for effect in observed_ss.keys()}

    print(f"\nRunning permutation tests with {n_permutations} permutations...")

    for perm in range(n_permutations):
        if perm % 100 == 0 and perm > 0:
            print(f"  Completed {perm} permutations")

        # Create permuted data
        perm_data = data.copy()

        # Permute each factor separately
        for effect in observed_ss.keys():
            if effect == 'C(treatment)':
                # Permute treatment labels
                perm_data['treatment'] = np.random.permutation(perm_data['treatment'].values)
            elif effect == 'C(timepoint)':
                # Permute timepoint labels
                perm_data['timepoint'] = np.random.permutation(perm_data['timepoint'].values)
            elif effect == 'C(treatment):C(timepoint)':
                # Permute interaction by shuffling combinations
                combinations = perm_data[['treatment', 'timepoint']].values
                np.random.shuffle(combinations)
                perm_data['treatment'] = combinations[:, 0]
                perm_data['timepoint'] = combinations[:, 1]

        # Run ASCA on permuted data
        perm_asca = asca(perm_data, formula, design_cols, feature_cols)
        perm_effect_matrices = perm_asca['effect_matrices']

        # Calculate sum of squares for each permuted effect
        for effect in observed_ss.keys():
            perm_ss = np.sum(perm_effect_matrices[effect]**2)
            permuted_ss[effect].append(perm_ss)

    # Calculate p-values
    p_values = {}
    for effect in observed_ss.keys():
        # Count how many permuted SS are >= observed SS
        p_values[effect] = (np.sum(np.array(permuted_ss[effect]) >= observed_ss[effect]) + 1) / (n_permutations + 1)

    print("\nPermutation Test Results:")
    for effect, p_value in p_values.items():
        significance = "significant" if p_value < 0.05 else "not significant"
        print(f"{effect}: p-value = {p_value:.4f} ({significance})")

    # Add results to ASCA results
    asca_results['p_values'] = p_values
    asca_results['permuted_ss'] = permuted_ss

    return asca_results

# ============ PE-ASCA (Permutation-Enhanced ASCA) ============

def pe_asca(data, formula, design_cols, feature_cols, n_permutations=999, alpha=0.05):
    """
    Implement PE-ASCA (Permutation-Enhanced ASCA)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    formula: str
        ANOVA formula for decomposition
    design_cols: list
        List of column names for design factors
    feature_cols: list
        List of column names for features
    n_permutations: int
        Number of permutations for significance testing
    alpha: float
        Significance level

    Returns:
    --------
    Dictionary containing PE-ASCA results
    """
    print("\n=== PE-ASCA Analysis (Permutation-Enhanced ASCA) ===")

    # First, run ASCA+ to get permutation test results
    asca_plus_results = asca_plus(data, formula, design_cols, feature_cols, n_permutations)

    # Extract relevant information
    effect_matrices = asca_plus_results['effect_matrices']
    p_values = asca_plus_results['p_values']

    # Enhanced decomposition: only keep significant effects
    enhanced_matrices = {}
    for effect, p_value in p_values.items():
        if p_value <= alpha:
            enhanced_matrices[effect] = effect_matrices[effect]
            print(f"Effect {effect} is significant (p={p_value:.4f}) and included in enhanced model")
        else:
            # Add non-significant effects to residuals
            if 'Residual' not in enhanced_matrices:
                enhanced_matrices['Residual'] = effect_matrices[effect].copy()
            else:
                enhanced_matrices['Residual'] += effect_matrices[effect]
            print(f"Effect {effect} is not significant (p={p_value:.4f}) and added to residuals")

    # Add original residuals if not already included
    if 'Residual' not in enhanced_matrices:
        enhanced_matrices['Residual'] = effect_matrices['Residual']
    else:
        enhanced_matrices['Residual'] += effect_matrices['Residual']

    # Apply PCA to each significant effect matrix
    pca_results = {}
    explained_var = {}

    for effect, matrix in enhanced_matrices.items():
        if effect != 'Residual':  # Skip PCA on residuals
            pca = PCA()
            pca_results[effect] = pca.fit_transform(matrix)
            explained_var[effect] = pca.explained_variance_ratio_

            # Print effect summary
            effect_ss = np.sum(matrix**2)
            total_ss = np.sum([np.sum(m**2) for m in enhanced_matrices.values()])
            print(f"{effect} - Sum of Squares: {effect_ss:.4f} ({effect_ss/total_ss*100:.2f}% of total)")
            if len(explained_var[effect]) > 0:
                print(f"  PC1 explained variance: {explained_var[effect][0]*100:.2f}%")
                if len(explained_var[effect]) > 1:
                    print(f"  PC2 explained variance: {explained_var[effect][1]*100:.2f}%")

    # Create result structure
    pe_asca_results = {
        'effect_matrices': enhanced_matrices,
        'pca_results': pca_results,
        'explained_var': explained_var,
        'p_values': p_values,
        'total_variance': asca_plus_results['total_variance']
    }

    return pe_asca_results

# ============ LiMM-PCA (Linear Mixed Models with PCA) ============

def limm_pca(data, feature_cols, verbose=True):
    """
    Implement LiMM-PCA (Linear Mixed Models with PCA)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    feature_cols: list
        List of column names for features
    verbose: bool
        Whether to print detailed output

    Returns:
    --------
    Dictionary containing LiMM-PCA results
    """
    if verbose:
        print("\n=== LiMM-PCA Analysis (Linear Mixed Models with PCA) ===")

    # Extract data matrix
    X = data[feature_cols].values

    # Standardize the data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Initialize containers for results
    fixed_effects = {}
    random_effects = {}
    residuals = {}
    pvalues = {}

    # Apply mixed model to each feature
    for i, feature in enumerate(feature_cols):
        if verbose and i % 3 == 0:
            print(f"Processing feature {i+1}/{len(feature_cols)}...")

        # Create mixed model formula
        formula = f"{feature} ~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"

        # Fit mixed effects model
        try:
            # Convert categorical variables to string to ensure proper handling
            data_copy = data.copy()
            if isinstance(data_copy['treatment'].iloc[0], str) == False:
                data_copy['treatment'] = data_copy['treatment'].astype(str)
            if isinstance(data_copy['timepoint'].iloc[0], str) == False:
                data_copy['timepoint'] = data_copy['timepoint'].astype(str)
            if isinstance(data_copy['subject_id'].iloc[0], str) == False:
                data_copy['subject_id'] = data_copy['subject_id'].astype(str)

            model = mixedlm(formula, data_copy, groups=data_copy['subject_id'])
            result = model.fit(reml=True)

            # Extract fixed effects
            fixed_pred = result.fittedvalues - result.random_effects[data_copy['subject_id'].values].flatten()
            fixed_effects[feature] = fixed_pred

            # Extract random effects (subject)
            subject_effect = np.zeros(len(data_copy))
            for idx, subj in enumerate(data_copy['subject_id']):
                subject_effect[idx] = result.random_effects[subj][0]
            random_effects[feature] = subject_effect

            # Extract residuals
            residuals[feature] = result.resid

            # Store p-values for fixed effects
            pvalues[feature] = result.pvalues

        except Exception as e:
            if verbose:
                print(f"Error fitting model for {feature}: {e}")
            # Use simpler approach if model fails
            fixed_effects[feature] = np.zeros(len(data))
            random_effects[feature] = np.zeros(len(data))
            residuals[feature] = X_scaled[:, i]

    # Convert to matrices
    fixed_matrix = np.column_stack([fixed_effects[feature] for feature in feature_cols])
    random_matrix = np.column_stack([random_effects[feature] for feature in feature_cols])
    resid_matrix = np.column_stack([residuals[feature] for feature in feature_cols])

    # Calculate variance explained
    total_var = np.sum(X_scaled**2)
    fixed_var = np.sum(fixed_matrix**2)
    random_var = np.sum(random_matrix**2)
    resid_var = np.sum(resid_matrix**2)

    if verbose:
        print("\nVariance Partition:")
        print(f"Fixed Effects: {fixed_var/total_var*100:.2f}%")
        print(f"Random Effects: {random_var/total_var*100:.2f}%")
        print(f"Residuals: {resid_var/total_var*100:.2f}%")

    # Apply PCA to fixed effects matrix
    pca_fixed = PCA()
    pca_fixed.fit(fixed_matrix)

    if verbose:
        print("\nPCA on Fixed Effects:")
        print(f"PC1 explained variance: {pca_fixed.explained_variance_ratio_[0]*100:.2f}%")
        if len(pca_fixed.explained_variance_ratio_) > 1:
            print(f"PC2 explained variance: {pca_fixed.explained_variance_ratio_[1]*100:.2f}%")

    # Store results
    results = {
        'fixed_matrix': fixed_matrix,
        'random_matrix': random_matrix,
        'resid_matrix': resid_matrix,
        'pca_fixed': pca_fixed,
        'fixed_scores': pca_fixed.transform(fixed_matrix),
        'total_var': total_var,
        'fixed_var': fixed_var,
        'random_var': random_var,
        'resid_var': resid_var
    }

    return results

# ============ MSCA (Multilevel Simultaneous Component Analysis) ============

def msca(data, feature_cols, verbose=True):
    """
    Implement MSCA (Multilevel Simultaneous Component Analysis)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    feature_cols: list
        List of column names for features
    verbose: bool
        Whether to print detailed output

    Returns:
    --------
    Dictionary containing MSCA results
    """
    if verbose:
        print("\n=== MSCA Analysis (Multilevel Simultaneous Component Analysis) ===")

    # Extract data
    X = data[feature_cols].values
    subjects = data['subject_id'].unique()
    n_subjects = len(subjects)

    # Center the data
    X_centered = X - np.mean(X, axis=0)

    # Split data into between-subject and within-subject parts
    X_between = np.zeros_like(X)
    X_within = np.zeros_like(X)

    # Calculate between-subject variation (level 2)
    for subject in subjects:
        subject_indices = data['subject_id'] == subject
        subject_mean = np.mean(X[subject_indices], axis=0)
        X_between[subject_indices] = subject_mean

    # Calculate within-subject variation (level 1)
    X_within = X - X_between

    # Calculate variance explained by each level
    total_var = np.sum(X_centered**2)
    between_var = np.sum(X_between**2)
    within_var = np.sum(X_within**2)

    if verbose:
        print("\nVariance Partition:")
        print(f"Between-subject: {between_var/total_var*100:.2f}%")
        print(f"Within-subject: {within_var/total_var*100:.2f}%")

    # Apply PCA to between-subject variation
    pca_between = PCA()
    between_scores = pca_between.fit_transform(X_between)

    # Apply PCA to within-subject variation
    pca_within = PCA()
    within_scores = pca_within.fit_transform(X_within)

    if verbose:
        print("\nPCA on Between-subject Variation:")
        print(f"PC1 explained variance: {pca_between.explained_variance_ratio_[0]*100:.2f}%")
        if len(pca_between.explained_variance_ratio_) > 1:
            print(f"PC2 explained variance: {pca_between.explained_variance_ratio_[1]*100:.2f}%")

        print("\nPCA on Within-subject Variation:")
        print(f"PC1 explained variance: {pca_within.explained_variance_ratio_[0]*100:.2f}%")
        if len(pca_within.explained_variance_ratio_) > 1:
            print(f"PC2 explained variance: {pca_within.explained_variance_ratio_[1]*100:.2f}%")

    # Store results
    results = {
        'X_between': X_between,
        'X_within': X_within,
        'pca_between': pca_between,
        'pca_within': pca_within,
        'between_scores': between_scores,
        'within_scores': within_scores,
        'total_var': total_var,
        'between_var': between_var,
        'within_var': within_var
    }

    return results

# Run all methods on the synthetic data

print("\n==== RUNNING ALL METHODS ON THE SYNTHETIC DATASET ====")

# Prepare formula and design columns for ASCA methods
formula = "~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"
design_cols = ['treatment', 'timepoint', 'subject_id']

# Run each method
print("\n===== METHOD 1: ASCA =====")
asca_results = asca(X_matrix, formula, design_cols, feature_cols)

print("\n===== METHOD 2: ASCA+ =====")
asca_plus_results = asca_plus(X_matrix, formula, design_cols, feature_cols, n_permutations=99)  # Reduced for computation

print("\n===== METHOD 3: PE-ASCA =====")
pe_asca_results = pe_asca(X_matrix, formula, design_cols, feature_cols, n_permutations=99, alpha=0.05)  # Reduced for computation

print("\n===== METHOD 4: LiMM-PCA =====")
limm_pca_results = limm_pca(X_matrix, feature_cols)

print("\n===== METHOD 5: MSCA =====")
msca_results = msca(X_matrix, feature_cols)

# Improved implementation of LiMM-PCA to fix the 'unhashable type: numpy.ndarray' error

def limm_pca_fixed(data, feature_cols, verbose=True):
    """
    Implement LiMM-PCA (Linear Mixed Models with PCA) with fixed implementation
    to resolve the 'unhashable type: numpy.ndarray' error.

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    feature_cols: list
        List of column names for features
    verbose: bool
        Whether to print detailed output

    Returns:
    --------
    Dictionary containing LiMM-PCA results
    """
    if verbose:
        print("\n=== LiMM-PCA Analysis (Linear Mixed Models with PCA) ===")

    # Extract data matrix
    X = data[feature_cols].values

    # Standardize the data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Create copy of data with proper string conversion for categorical variables
    data_copy = data.copy()

    # Convert categorical variables to strings
    data_copy['subject_id'] = data_copy['subject_id'].astype(str)
    data_copy['treatment'] = data_copy['treatment'].astype(str)
    data_copy['timepoint'] = data_copy['timepoint'].astype(str)

    # Initialize containers for results
    fixed_effects = {}
    random_effects = {}
    residuals = {}
    pvalues = {}

    # Apply mixed model to each feature
    for i, feature in enumerate(feature_cols):
        if verbose and i % 3 == 0:
            print(f"Processing feature {i+1}/{len(feature_cols)}...")

        # Create mixed model formula
        formula = f"{feature} ~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"

        # Fit mixed effects model
        try:
            # Fit the model with explicit specification of groups
            model = mixedlm(formula, data_copy, groups=data_copy['subject_id'])
            result = model.fit(reml=True)

            # Extract fixed effects (predicted values without random effects)
            subjects = data_copy['subject_id'].values
            re_dict = {str(subj): val[0] for subj, val in result.random_effects.items()}

            # Create array for random effects
            subject_effect = np.array([re_dict.get(str(subj), 0) for subj in subjects])

            # Fixed effects = fitted values - random effects
            fixed_effects[feature] = result.fittedvalues - subject_effect

            # Store random effects
            random_effects[feature] = subject_effect

            # Extract residuals
            residuals[feature] = result.resid

            # Store p-values for fixed effects
            pvalues[feature] = result.pvalues

        except Exception as e:
            if verbose:
                print(f"Error fitting model for {feature}: {e}")
            # Use simpler approach if model fails
            fixed_effects[feature] = np.zeros(len(data))
            random_effects[feature] = np.zeros(len(data))
            residuals[feature] = X_scaled[:, i]

    # Convert to matrices
    fixed_matrix = np.column_stack([fixed_effects[feature] for feature in feature_cols])
    random_matrix = np.column_stack([random_effects[feature] for feature in feature_cols])
    resid_matrix = np.column_stack([residuals[feature] for feature in feature_cols])

    # Calculate variance explained
    total_var = np.sum(X_scaled**2)
    fixed_var = np.sum(fixed_matrix**2)
    random_var = np.sum(random_matrix**2)
    resid_var = np.sum(resid_matrix**2)

    if verbose:
        print("\nVariance Partition:")
        print(f"Fixed Effects: {fixed_var/total_var*100:.2f}%")
        print(f"Random Effects: {random_var/total_var*100:.2f}%")
        print(f"Residuals: {resid_var/total_var*100:.2f}%")

    # Apply PCA to fixed effects matrix
    pca_fixed = PCA()
    pca_fixed.fit(fixed_matrix)

    if verbose and len(pca_fixed.explained_variance_ratio_) > 0:
        print("\nPCA on Fixed Effects:")
        print(f"PC1 explained variance: {pca_fixed.explained_variance_ratio_[0]*100:.2f}%")
        if len(pca_fixed.explained_variance_ratio_) > 1:
            print(f"PC2 explained variance: {pca_fixed.explained_variance_ratio_[1]*100:.2f}%")

    # Store results
    results = {
        'fixed_matrix': fixed_matrix,
        'random_matrix': random_matrix,
        'resid_matrix': resid_matrix,
        'pca_fixed': pca_fixed,
        'fixed_scores': pca_fixed.transform(fixed_matrix) if len(pca_fixed.explained_variance_ratio_) > 0 else np.array([]),
        'total_var': total_var,
        'fixed_var': fixed_var,
        'random_var': random_var,
        'resid_var': resid_var
    }

    return results

# Example usage:
# limm_pca_results = limm_pca_fixed(X_matrix, feature_cols)

# Install required packages (run this once)
!pip install -q numpy pandas matplotlib seaborn scikit-learn scipy statsmodels

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from scipy import stats
from scipy.linalg import svd
import statsmodels.api as sm
from statsmodels.formula.api import ols, mixedlm
from statsmodels.stats.anova import anova_lm
import random
from itertools import product
from sklearn.metrics import r2_score
from scipy.stats import pearsonr
import matplotlib.gridspec as gridspec
import warnings
warnings.filterwarnings('ignore')

# Set up plotting styles
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_context("notebook", font_scale=1.2)
np.random.seed(42)  # For reproducibility

def generate_synthetic_data(n_subjects=20, n_timepoints=3, n_features=10,
                          n_treatments=2, effect_size=0.5, random_effect_size=0.3,
                          noise_level=0.2):
    """Generate synthetic multivariate longitudinal data"""
    # Create empty dataframe
    columns = ['subject_id', 'treatment', 'timepoint'] + [f'feature_{i+1}' for i in range(n_features)]
    data = []

    # Treatment effects (different for each feature)
    treatment_effects = np.random.normal(0, effect_size, (n_treatments, n_features))

    # Timepoint effects (different for each feature)
    time_effects = np.random.normal(0, effect_size * 0.8, (n_timepoints, n_features))

    # Treatment x Time interaction effects
    interaction_effects = np.random.normal(0, effect_size * 0.5,
                                          (n_treatments, n_timepoints, n_features))

    # Generate data for each subject
    for subject in range(n_subjects):
        # Assign treatment (balanced design)
        treatment = subject % n_treatments

        # Random subject effect (consistent across timepoints)
        subject_effect = np.random.normal(0, random_effect_size, n_features)

        for timepoint in range(n_timepoints):
            # Base values for this subject at this timepoint
            feature_values = np.zeros(n_features)

            # Add treatment effect
            feature_values += treatment_effects[treatment]

            # Add time effect
            feature_values += time_effects[timepoint]

            # Add treatment x time interaction
            feature_values += interaction_effects[treatment, timepoint]

            # Add subject-specific effect
            feature_values += subject_effect

            # Add random noise
            feature_values += np.random.normal(0, noise_level, n_features)

            # Add row to data
            row = [f'S{subject+1}', f'T{treatment+1}', f'TP{timepoint+1}'] + feature_values.tolist()
            data.append(row)

    # Create DataFrame
    df = pd.DataFrame(data, columns=columns)

    # Convert features to float
    for i in range(n_features):
        df[f'feature_{i+1}'] = df[f'feature_{i+1}'].astype(float)

    return df

# Generate balanced dataset with 30 subjects, 3 timepoints, 10 features, 2 treatments
df = generate_synthetic_data(n_subjects=30, n_timepoints=3, n_features=10,
                          n_treatments=2, effect_size=0.7, random_effect_size=0.4,
                          noise_level=0.3)

# Extract feature columns
feature_cols = [col for col in df.columns if 'feature' in col]
X = df[feature_cols].values

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create a data matrix for easier handling
X_matrix = pd.DataFrame(X_scaled, columns=feature_cols)
X_matrix['subject_id'] = df['subject_id']
X_matrix['treatment'] = df['treatment']
X_matrix['timepoint'] = df['timepoint']

print("Data prepared for analysis.")
print("Number of samples:", X_matrix.shape[0])
print("Number of features:", len(feature_cols))
print("Factors: treatment, timepoint, subject_id")

# Visualize the data
plt.figure(figsize=(12, 8))
feature_data = df[feature_cols].values

# Apply PCA for visualization
pca = PCA(n_components=2)
pca_result = pca.fit_transform(X_scaled)

# Add PCA results to the dataframe for plotting
df_plot = df.copy()
df_plot['PC1'] = pca_result[:, 0]
df_plot['PC2'] = pca_result[:, 1]

# Plot by treatment and timepoint
plt.figure(figsize=(15, 6))

plt.subplot(1, 2, 1)
sns.scatterplot(data=df_plot, x='PC1', y='PC2', hue='treatment', style='timepoint', s=80, alpha=0.7)
plt.title('PCA of Data by Treatment and Timepoint')
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

plt.subplot(1, 2, 2)
# Correlation heatmap of features
corr = df[feature_cols].corr()
sns.heatmap(corr, annot=False, cmap='coolwarm', center=0)
plt.title('Feature Correlation Heatmap')

plt.tight_layout()
plt.show()

"""Method 1: ASCA (ANOVA-Simultaneous Component Analysis)"""

def asca(data, formula, design_cols, feature_cols):
    """
    Implement ASCA (ANOVA-Simultaneous Component Analysis)
    """
    print("\n=== ASCA Analysis ===")

    # Extract design and data matrices
    X = data[feature_cols].values
    n_samples, n_features = X.shape

    # Center the data
    X_centered = X - np.mean(X, axis=0)

    # Fit the ANOVA model for each feature
    effect_matrices = {}
    anova_summaries = []

    # ANOVA decomposition
    for feature_idx, feature in enumerate(feature_cols):
        # Create formula for this feature
        feature_formula = f"{feature} ~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"

        # Fit ANOVA model
        model = ols(feature_formula, data=data).fit()
        anova_table = sm.stats.anova_lm(model, typ=2)

        # Store ANOVA summary for this feature
        if feature_idx == 0:  # Only store once
            anova_summaries.append(anova_table)

    # Extract effects from ANOVA model
    # Create empty matrices for each effect
    effects = ['C(treatment)', 'C(timepoint)', 'C(treatment):C(timepoint)', 'Residual']
    effect_matrices = {effect: np.zeros((n_samples, n_features)) for effect in effects}

    # Fill effect matrices feature by feature
    for feature_idx, feature in enumerate(feature_cols):
        # Create formula for this feature
        feature_formula = f"{feature} ~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"

        # Fit ANOVA model for this feature
        model = ols(feature_formula, data=data).fit()

        # Get predicted values for each effect
        # Treatment effect
        treatment_effect = np.zeros(n_samples)
        for idx, row in data.iterrows():
            treatment = row['treatment']
            param_name = f"C(treatment)[T.{treatment}]"
            if param_name in model.params:
                treatment_effect[idx] = model.params[param_name]
        effect_matrices['C(treatment)'][:, feature_idx] = treatment_effect

        # Timepoint effect
        timepoint_effect = np.zeros(n_samples)
        for idx, row in data.iterrows():
            timepoint = row['timepoint']
            param_name = f"C(timepoint)[T.{timepoint}]"
            if param_name in model.params:
                timepoint_effect[idx] = model.params[param_name]
        effect_matrices['C(timepoint)'][:, feature_idx] = timepoint_effect

        # Interaction effect
        interaction_effect = np.zeros(n_samples)
        for idx, row in data.iterrows():
            treatment = row['treatment']
            timepoint = row['timepoint']
            param_name = f"C(treatment)[T.{treatment}]:C(timepoint)[T.{timepoint}]"
            if param_name in model.params:
                interaction_effect[idx] = model.params[param_name]
        effect_matrices['C(treatment):C(timepoint)'][:, feature_idx] = interaction_effect

        # Residual effect
        effect_matrices['Residual'][:, feature_idx] = model.resid

    # Apply PCA to each effect matrix
    pca_results = {}
    explained_var = {}

    for effect, matrix in effect_matrices.items():
        if effect != 'Residual':  # Skip PCA on residuals
            pca = PCA()
            pca_results[effect] = pca.fit_transform(matrix)
            explained_var[effect] = pca.explained_variance_ratio_

            # Print effect summary
            effect_ss = np.sum(matrix**2)
            print(f"{effect} - Sum of Squares: {effect_ss:.4f}")
            if len(explained_var[effect]) > 0:
                print(f"  PC1 explained variance: {explained_var[effect][0]*100:.2f}%")
                if len(explained_var[effect]) > 1:
                    print(f"  PC2 explained variance: {explained_var[effect][1]*100:.2f}%")

    # Calculate total variance
    total_variance = np.sum(X_centered**2)

    return {
        'effect_matrices': effect_matrices,
        'pca_results': pca_results,
        'explained_var': explained_var,
        'total_variance': total_variance,
        'anova_summaries': anova_summaries
    }

# Prepare formula and design columns
formula = "~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"
design_cols = ['treatment', 'timepoint', 'subject_id']

# Run ASCA analysis
asca_results = asca(X_matrix, formula, design_cols, feature_cols)

# Visualize ASCA results
plt.figure(figsize=(18, 6))

# Treatment effect
plt.subplot(1, 3, 1)
if 'C(treatment)' in asca_results['pca_results']:
    treatment_pc = asca_results['pca_results']['C(treatment)'][:, 0]
    sns.boxplot(x=X_matrix['treatment'], y=treatment_pc)
    plt.title('Treatment Effect (PC1)')
    plt.xlabel('Treatment')
    plt.ylabel('PC1 Score')

# Timepoint effect
plt.subplot(1, 3, 2)
if 'C(timepoint)' in asca_results['pca_results']:
    timepoint_pc = asca_results['pca_results']['C(timepoint)'][:, 0]
    sns.boxplot(x=X_matrix['timepoint'], y=timepoint_pc)
    plt.title('Timepoint Effect (PC1)')
    plt.xlabel('Timepoint')
    plt.ylabel('PC1 Score')

# Interaction effect
plt.subplot(1, 3, 3)
if 'C(treatment):C(timepoint)' in asca_results['pca_results']:
    interaction_pc = asca_results['pca_results']['C(treatment):C(timepoint)'][:, 0]
    sns.boxplot(x=X_matrix['treatment'], y=interaction_pc, hue=X_matrix['timepoint'])
    plt.title('Treatment × Timepoint Interaction (PC1)')
    plt.xlabel('Treatment')
    plt.ylabel('PC1 Score')
    plt.legend(title='Timepoint')

plt.tight_layout()
plt.show()

"""Method 2: ASCA+ (ASCA with Permutation Testing)"""

def asca_plus(data, formula, design_cols, feature_cols, n_permutations=99):
    """
    Implement ASCA+ (ASCA with permutation testing)

    Note: For real analyses, increase n_permutations to 999 or more
    """
    print("\n=== ASCA+ Analysis (ASCA with Permutation Testing) ===")

    # Run standard ASCA
    asca_results = asca(data, formula, design_cols, feature_cols)

    # Get effect matrices and calculate sum of squares for each effect
    effect_matrices = asca_results['effect_matrices']
    observed_ss = {effect: np.sum(matrix**2) for effect, matrix in effect_matrices.items()
                 if effect != 'Residual'}

    # Permutation testing
    permuted_ss = {effect: [] for effect in observed_ss.keys()}

    print(f"\nRunning permutation tests with {n_permutations} permutations...")

    for perm in range(n_permutations):
        if perm % 20 == 0 and perm > 0:
            print(f"  Completed {perm} permutations")

        # Create permuted data
        perm_data = data.copy()

        # Permute each factor separately
        for effect in observed_ss.keys():
            if effect == 'C(treatment)':
                # Permute treatment labels
                perm_data['treatment'] = np.random.permutation(perm_data['treatment'].values)
            elif effect == 'C(timepoint)':
                # Permute timepoint labels
                perm_data['timepoint'] = np.random.permutation(perm_data['timepoint'].values)
            elif effect == 'C(treatment):C(timepoint)':
                # Permute interaction by shuffling combinations
                combinations = perm_data[['treatment', 'timepoint']].values
                np.random.shuffle(combinations)
                perm_data['treatment'] = combinations[:, 0]
                perm_data['timepoint'] = combinations[:, 1]

        # Run ASCA on permuted data
        perm_asca = asca(perm_data, formula, design_cols, feature_cols)
        perm_effect_matrices = perm_asca['effect_matrices']

        # Calculate sum of squares for each permuted effect
        for effect in observed_ss.keys():
            perm_ss = np.sum(perm_effect_matrices[effect]**2)
            permuted_ss[effect].append(perm_ss)

    # Calculate p-values
    p_values = {}
    for effect in observed_ss.keys():
        # Count how many permuted SS are >= observed SS
        p_values[effect] = (np.sum(np.array(permuted_ss[effect]) >= observed_ss[effect]) + 1) / (n_permutations + 1)

    print("\nPermutation Test Results:")
    for effect, p_value in p_values.items():
        significance = "significant" if p_value < 0.05 else "not significant"
        print(f"{effect}: p-value = {p_value:.4f} ({significance})")

    # Add results to ASCA results
    asca_results['p_values'] = p_values
    asca_results['permuted_ss'] = permuted_ss

    # Visualize permutation test results
    plt.figure(figsize=(15, 5))

    for i, effect in enumerate(observed_ss.keys()):
        plt.subplot(1, 3, i+1)
        sns.histplot(permuted_ss[effect], bins=20, kde=True)
        plt.axvline(observed_ss[effect], color='red', linestyle='--',
                    label=f'Observed ({p_values[effect]:.3f})')
        plt.title(f'{effect} Permutation Test')
        plt.xlabel('Sum of Squares')
        plt.ylabel('Frequency')
        plt.legend()

    plt.tight_layout()
    plt.show()

    return asca_results

# Run ASCA+ with fewer permutations for demonstration
# For real analyses, use n_permutations=999 or more
asca_plus_results = asca_plus(X_matrix, formula, design_cols, feature_cols, n_permutations=99)

# Continued from PE-ASCA implementation

def pe_asca(data, formula, design_cols, feature_cols, n_permutations=99, alpha=0.05):
    """
    Implement PE-ASCA (Permutation-Enhanced ASCA)

    Note: For real analyses, increase n_permutations to 999 or more
    """
    print("\n=== PE-ASCA Analysis (Permutation-Enhanced ASCA) ===")

    # First, run ASCA+ to get permutation test results
    asca_plus_results = asca_plus(data, formula, design_cols, feature_cols, n_permutations)

    # Extract relevant information
    effect_matrices = asca_plus_results['effect_matrices']
    p_values = asca_plus_results['p_values']

    # Enhanced decomposition: only keep significant effects
    enhanced_matrices = {}
    for effect, p_value in p_values.items():
        if p_value <= alpha:
            enhanced_matrices[effect] = effect_matrices[effect]
            print(f"Effect {effect} is significant (p={p_value:.4f}) and included in enhanced model")
        else:
            # Add non-significant effects to residuals
            if 'Residual' not in enhanced_matrices:
                enhanced_matrices['Residual'] = effect_matrices[effect].copy()
            else:
                enhanced_matrices['Residual'] += effect_matrices[effect]
            print(f"Effect {effect} is not significant (p={p_value:.4f}) and added to residuals")

    # Add original residuals if not already included
    if 'Residual' not in enhanced_matrices:
        enhanced_matrices['Residual'] = effect_matrices['Residual']
    else:
        enhanced_matrices['Residual'] += effect_matrices['Residual']

    # Apply PCA to each significant effect matrix
    pca_results = {}
    explained_var = {}

    for effect, matrix in enhanced_matrices.items():
        if effect != 'Residual':  # Skip PCA on residuals
            pca = PCA()
            pca_results[effect] = pca.fit_transform(matrix)
            explained_var[effect] = pca.explained_variance_ratio_

            # Print effect summary
            effect_ss = np.sum(matrix**2)
            total_ss = np.sum([np.sum(m**2) for m in enhanced_matrices.values()])
            print(f"{effect} - Sum of Squares: {effect_ss:.4f} ({effect_ss/total_ss*100:.2f}% of total)")
            if len(explained_var[effect]) > 0:
                print(f"  PC1 explained variance: {explained_var[effect][0]*100:.2f}%")
                if len(explained_var[effect]) > 1:
                    print(f"  PC2 explained variance: {explained_var[effect][1]*100:.2f}%")

    # Create result structure
    pe_asca_results = {
        'effect_matrices': enhanced_matrices,
        'pca_results': pca_results,
        'explained_var': explained_var,
        'p_values': p_values,
        'total_variance': asca_plus_results['total_variance']
    }

    # Visualize PE-ASCA results
    plt.figure(figsize=(12, 5))

    plot_idx = 1
    for effect in enhanced_matrices.keys():
        if effect != 'Residual' and effect in pca_results:
            plt.subplot(1, len(pca_results), plot_idx)

            # Get PC scores
            pc_scores = pca_results[effect][:, 0]

            # Plot based on the effect type
            if effect == 'C(treatment)':
                sns.boxplot(x=data['treatment'], y=pc_scores)
                plt.title(f'PE-ASCA: {effect} (p={p_values[effect]:.3f})')
                plt.xlabel('Treatment')
            elif effect == 'C(timepoint)':
                sns.boxplot(x=data['timepoint'], y=pc_scores)
                plt.title(f'PE-ASCA: {effect} (p={p_values[effect]:.3f})')
                plt.xlabel('Timepoint')
            else:
                # For interaction, create a combined factor
                combined = data['treatment'].astype(str) + '-' + data['timepoint'].astype(str)
                sns.boxplot(x=combined, y=pc_scores)
                plt.title(f'PE-ASCA: {effect} (p={p_values[effect]:.3f})')
                plt.xlabel('Treatment-Timepoint')
                plt.xticks(rotation=45)

            plt.ylabel('PC1 Score')
            plot_idx += 1

    plt.tight_layout()
    plt.show()

    return pe_asca_results

# Run PE-ASCA
pe_asca_results = pe_asca(X_matrix, formula, design_cols, feature_cols, n_permutations=99, alpha=0.05)

# Method 4: LiMM-PCA (Linear Mixed Models with PCA)

def limm_pca(data, feature_cols, verbose=True):
    """
    Implement LiMM-PCA (Linear Mixed Models with PCA)
    """
    if verbose:
        print("\n=== LiMM-PCA Analysis (Linear Mixed Models with PCA) ===")

    # Extract data matrix
    X = data[feature_cols].values

    # Standardize the data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Create copy of data with proper string conversion for categorical variables
    data_copy = data.copy()

    # Convert categorical variables to strings
    data_copy['subject_id'] = data_copy['subject_id'].astype(str)
    data_copy['treatment'] = data_copy['treatment'].astype(str)
    data_copy['timepoint'] = data_copy['timepoint'].astype(str)

    # Initialize containers for results
    fixed_effects = {}
    random_effects = {}
    residuals = {}
    pvalues = {}

    # Apply mixed model to each feature
    for i, feature in enumerate(feature_cols):
        if verbose and i % 3 == 0:
            print(f"Processing feature {i+1}/{len(feature_cols)}...")

        # Create mixed model formula
        formula = f"{feature} ~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"

        # Fit mixed effects model
        try:
            # Fit the model with explicit specification of groups
            model = mixedlm(formula, data_copy, groups=data_copy['subject_id'])
            result = model.fit(reml=True)

            # Extract fixed effects (predicted values without random effects)
            subjects = data_copy['subject_id'].values
            re_dict = {str(subj): val[0] for subj, val in result.random_effects.items()}

            # Create array for random effects
            subject_effect = np.array([re_dict.get(str(subj), 0) for subj in subjects])

            # Fixed effects = fitted values - random effects
            fixed_effects[feature] = result.fittedvalues - subject_effect

            # Store random effects
            random_effects[feature] = subject_effect

            # Extract residuals
            residuals[feature] = result.resid

            # Store p-values for fixed effects
            pvalues[feature] = result.pvalues

        except Exception as e:
            if verbose:
                print(f"Error fitting model for {feature}: {e}")
            # Use simpler approach if model fails
            fixed_effects[feature] = np.zeros(len(data))
            random_effects[feature] = np.zeros(len(data))
            residuals[feature] = X_scaled[:, i]

    # Convert to matrices
    fixed_matrix = np.column_stack([fixed_effects[feature] for feature in feature_cols])
    random_matrix = np.column_stack([random_effects[feature] for feature in feature_cols])
    resid_matrix = np.column_stack([residuals[feature] for feature in feature_cols])

    # Calculate variance explained
    total_var = np.sum(X_scaled**2)
    fixed_var = np.sum(fixed_matrix**2)
    random_var = np.sum(random_matrix**2)
    resid_var = np.sum(resid_matrix**2)

    if verbose:
        print("\nVariance Partition:")
        print(f"Fixed Effects: {fixed_var/total_var*100:.2f}%")
        print(f"Random Effects: {random_var/total_var*100:.2f}%")
        print(f"Residuals: {resid_var/total_var*100:.2f}%")

    # Apply PCA to fixed effects matrix
    pca_fixed = PCA()
    pca_fixed.fit(fixed_matrix)
    fixed_scores = pca_fixed.transform(fixed_matrix)

    if verbose and len(pca_fixed.explained_variance_ratio_) > 0:
        print("\nPCA on Fixed Effects:")
        print(f"PC1 explained variance: {pca_fixed.explained_variance_ratio_[0]*100:.2f}%")
        if len(pca_fixed.explained_variance_ratio_) > 1:
            print(f"PC2 explained variance: {pca_fixed.explained_variance_ratio_[1]*100:.2f}%")

    # Visualize results
    plt.figure(figsize=(15, 5))

    # Plot PC1 scores by treatment
    plt.subplot(1, 3, 1)
    sns.boxplot(x=data['treatment'], y=fixed_scores[:, 0])
    plt.title('LiMM-PCA: Treatment Effect (PC1)')
    plt.xlabel('Treatment')
    plt.ylabel('PC1 Score')

    # Plot PC1 scores by timepoint
    plt.subplot(1, 3, 2)
    sns.boxplot(x=data['timepoint'], y=fixed_scores[:, 0])
    plt.title('LiMM-PCA: Timepoint Effect (PC1)')
    plt.xlabel('Timepoint')
    plt.ylabel('PC1 Score')

    # Plot variance partition
    plt.subplot(1, 3, 3)
    var_df = pd.DataFrame({
        'Component': ['Fixed Effects', 'Random Effects', 'Residuals'],
        'Variance (%)': [fixed_var/total_var*100, random_var/total_var*100, resid_var/total_var*100]
    })
    sns.barplot(x='Component', y='Variance (%)', data=var_df)
    plt.title('LiMM-PCA: Variance Partition')
    plt.ylim(0, 100)
    plt.xticks(rotation=45)

    plt.tight_layout()
    plt.show()

    # Store results
    results = {
        'fixed_matrix': fixed_matrix,
        'random_matrix': random_matrix,
        'resid_matrix': resid_matrix,
        'pca_fixed': pca_fixed,
        'fixed_scores': fixed_scores,
        'total_var': total_var,
        'fixed_var': fixed_var,
        'random_var': random_var,
        'resid_var': resid_var
    }

    return results

# Run LiMM-PCA
limm_pca_results = limm_pca(X_matrix, feature_cols)

# Method 5: MSCA (Multilevel Simultaneous Component Analysis)

def msca(data, feature_cols, verbose=True):
    """
    Implement MSCA (Multilevel Simultaneous Component Analysis)
    """
    if verbose:
        print("\n=== MSCA Analysis (Multilevel Simultaneous Component Analysis) ===")

    # Extract data
    X = data[feature_cols].values
    subjects = data['subject_id'].unique()
    n_subjects = len(subjects)

    # Center the data
    X_centered = X - np.mean(X, axis=0)

    # Split data into between-subject and within-subject parts
    X_between = np.zeros_like(X)
    X_within = np.zeros_like(X)

    # Calculate between-subject variation (level 2)
    for subject in subjects:
        subject_indices = data['subject_id'] == subject
        subject_mean = np.mean(X[subject_indices], axis=0)
        X_between[subject_indices] = subject_mean

    # Calculate within-subject variation (level 1)
    X_within = X - X_between

    # Calculate variance explained by each level
    total_var = np.sum(X_centered**2)
    between_var = np.sum(X_between**2)
    within_var = np.sum(X_within**2)

    if verbose:
        print("\nVariance Partition:")
        print(f"Between-subject: {between_var/total_var*100:.2f}%")
        print(f"Within-subject: {within_var/total_var*100:.2f}%")

    # Apply PCA to between-subject variation
    pca_between = PCA()
    between_scores = pca_between.fit_transform(X_between)

    # Apply PCA to within-subject variation
    pca_within = PCA()
    within_scores = pca_within.fit_transform(X_within)

    if verbose:
        print("\nPCA on Between-subject Variation:")
        print(f"PC1 explained variance: {pca_between.explained_variance_ratio_[0]*100:.2f}%")
        if len(pca_between.explained_variance_ratio_) > 1:
            print(f"PC2 explained variance: {pca_between.explained_variance_ratio_[1]*100:.2f}%")

        print("\nPCA on Within-subject Variation:")
        print(f"PC1 explained variance: {pca_within.explained_variance_ratio_[0]*100:.2f}%")
        if len(pca_within.explained_variance_ratio_) > 1:
            print(f"PC2 explained variance: {pca_within.explained_variance_ratio_[1]*100:.2f}%")

    # Visualize MSCA results
    plt.figure(figsize=(15, 10))

    # Plot between-subject PC1 vs PC2
    plt.subplot(2, 2, 1)
    sns.scatterplot(x=between_scores[:, 0], y=between_scores[:, 1],
                    hue=data['treatment'], style=data['subject_id'], s=80)
    plt.title('Between-subject: PC1 vs PC2')
    plt.xlabel(f'PC1 ({pca_between.explained_variance_ratio_[0]*100:.1f}%)')
    plt.ylabel(f'PC2 ({pca_between.explained_variance_ratio_[1]*100:.1f}%)')
    plt.legend(title='Treatment', loc='upper right')

    # Plot within-subject PC1 vs PC2
    plt.subplot(2, 2, 2)
    sns.scatterplot(x=within_scores[:, 0], y=within_scores[:, 1],
                    hue=data['timepoint'], style=data['subject_id'], s=80)
    plt.title('Within-subject: PC1 vs PC2')
    plt.xlabel(f'PC1 ({pca_within.explained_variance_ratio_[0]*100:.1f}%)')
    plt.ylabel(f'PC2 ({pca_within.explained_variance_ratio_[1]*100:.1f}%)')
    plt.legend(title='Timepoint', loc='upper right')

    # Plot between-subject PC1 by treatment
    plt.subplot(2, 2, 3)
    sns.boxplot(x=data['treatment'], y=between_scores[:, 0])
    plt.title('Between-subject PC1 by Treatment')
    plt.xlabel('Treatment')
    plt.ylabel('PC1 Score')

    # Plot within-subject PC1 by timepoint
    plt.subplot(2, 2, 4)
    sns.boxplot(x=data['timepoint'], y=within_scores[:, 0])
    plt.title('Within-subject PC1 by Timepoint')
    plt.xlabel('Timepoint')
    plt.ylabel('PC1 Score')

    plt.tight_layout()
    plt.show()

    # Store results
    results = {
        'X_between': X_between,
        'X_within': X_within,
        'pca_between': pca_between,
        'pca_within': pca_within,
        'between_scores': between_scores,
        'within_scores': within_scores,
        'total_var': total_var,
        'between_var': between_var,
        'within_var': within_var
    }

    return results

# Run MSCA analysis
msca_results = msca(X_matrix, feature_cols)

# Method Comparison

def compare_methods(df, asca_results, asca_plus_results, pe_asca_results, limm_pca_results, msca_results):
    """
    Comprehensive comparison of all multivariate methods
    """
    print("\n==== COMPARATIVE ANALYSIS OF MULTIVARIATE METHODS ====")

    # 1. Setup figure for visualization
    fig = plt.figure(figsize=(20, 22))
    gs = gridspec.GridSpec(4, 2, height_ratios=[1, 1, 1, 1])

    # 2. Compare variance decomposition across methods
    print("\n1. Variance Decomposition Comparison:")

    # Extract variance components
    variances = {
        'ASCA': {
            'Treatment': np.sum(asca_results['effect_matrices']['C(treatment)']**2) / asca_results['total_variance'],
            'Timepoint': np.sum(asca_results['effect_matrices']['C(timepoint)']**2) / asca_results['total_variance'],
            'Interaction': np.sum(asca_results['effect_matrices']['C(treatment):C(timepoint)']**2) / asca_results['total_variance'],
            'Residual': np.sum(asca_results['effect_matrices']['Residual']**2) / asca_results['total_variance']
        }
    }

    # Add PE-ASCA variance components (might be different due to non-significant effects)
    variances['PE-ASCA'] = {}
    for effect in asca_results['effect_matrices'].keys():
        if effect in pe_asca_results['effect_matrices']:
            variances['PE-ASCA'][effect.replace('C(', '').replace(')', '')] = (
                np.sum(pe_asca_results['effect_matrices'][effect]**2) / pe_asca_results['total_variance']
            )
        else:
            variances['PE-ASCA'][effect.replace('C(', '').replace(')', '')] = 0

    # Add LiMM-PCA variance components
    variances['LiMM-PCA'] = {
        'Fixed Effects': limm_pca_results['fixed_var'] / limm_pca_results['total_var'],
        'Random Effects': limm_pca_results['random_var'] / limm_pca_results['total_var'],
        'Residual': limm_pca_results['resid_var'] / limm_pca_results['total_var']
    }

    # Add MSCA variance components
    variances['MSCA'] = {
        'Between-subject': msca_results['between_var'] / msca_results['total_var'],
        'Within-subject': msca_results['within_var'] / msca_results['total_var']
    }

    # Create variance decomposition dataframe
    variance_df = pd.DataFrame(columns=['Method', 'Component', 'Variance Percentage'])

    for method, components in variances.items():
        for component, value in components.items():
            variance_df = pd.concat([variance_df, pd.DataFrame({
                'Method': [method],
                'Component': [component],
                'Variance Percentage': [value * 100]
            })], ignore_index=True)

    # Plot variance decomposition
    ax1 = plt.subplot(gs[0, :])
    sns.barplot(x='Method', y='Variance Percentage', hue='Component', data=variance_df, ax=ax1)
    ax1.set_title('Variance Decomposition Across Methods', fontsize=14)
    ax1.set_ylabel('Percentage of Total Variance (%)')
    ax1.legend(title='Component', bbox_to_anchor=(1.05, 1), loc='upper left')

    # Print variance decomposition table
    print(variance_df.pivot(index='Component', columns='Method', values='Variance Percentage').round(2))

    plt.tight_layout()
    plt.show()

    return variance_df

# Only run this if you've successfully run all methods
try:
    variance_df = compare_methods(df, asca_results, asca_plus_results, pe_asca_results, limm_pca_results, msca_results)
except Exception as e:
    print(f"Could not complete method comparison: {e}")
    print("Make sure you've successfully run all methods before comparison.")

# Exporting Results

def export_results_summary(df, asca_results, pe_asca_results, msca_results):
    """Export summary of results to CSV files"""

    # 1. Variance decomposition summary
    variance_summary = pd.DataFrame({
        'Component': ['Treatment', 'Timepoint', 'Interaction', 'Between-subject', 'Within-subject', 'Residual'],
        'ASCA_pct': [
            np.sum(asca_results['effect_matrices']['C(treatment)']**2) / asca_results['total_variance'] * 100,
            np.sum(asca_results['effect_matrices']['C(timepoint)']**2) / asca_results['total_variance'] * 100,
            np.sum(asca_results['effect_matrices']['C(treatment):C(timepoint)']**2) / asca_results['total_variance'] * 100,
            np.nan,
            np.nan,
            np.sum(asca_results['effect_matrices']['Residual']**2) / asca_results['total_variance'] * 100
        ],
        'PE-ASCA_pct': [
            np.sum(pe_asca_results['effect_matrices'].get('C(treatment)', np.zeros_like(df[feature_cols].values))**2) / pe_asca_results['total_variance'] * 100,
            np.sum(pe_asca_results['effect_matrices'].get('C(timepoint)', np.zeros_like(df[feature_cols].values))**2) / pe_asca_results['total_variance'] * 100,
            np.sum(pe_asca_results['effect_matrices'].get('C(treatment):C(timepoint)', np.zeros_like(df[feature_cols].values))**2) / pe_asca_results['total_variance'] * 100,
            np.nan,
            np.nan,
            np.sum(pe_asca_results['effect_matrices']['Residual']**2) / pe_asca_results['total_variance'] * 100
        ],
        'MSCA_pct': [
            np.nan,
            np.nan,
            np.nan,
            msca_results['between_var'] / msca_results['total_var'] * 100,
            msca_results['within_var'] / msca_results['total_var'] * 100,
            np.nan
        ]
    })

    # 2. PCA explained variance summary
    pca_summary = pd.DataFrame({
        'Component': ['Treatment_PC1', 'Treatment_PC2', 'Timepoint_PC1', 'Timepoint_PC2',
                     'Interaction_PC1', 'Interaction_PC2', 'Between_PC1', 'Between_PC2',
                     'Within_PC1', 'Within_PC2'],
        'ASCA_pct': [
            asca_results['explained_var']['C(treatment)'][0] * 100 if len(asca_results['explained_var']['C(treatment)']) > 0 else np.nan,
            asca_results['explained_var']['C(treatment)'][1] * 100 if len(asca_results['explained_var']['C(treatment)']) > 1 else np.nan,
            asca_results['explained_var']['C(timepoint)'][0] * 100 if len(asca_results['explained_var']['C(timepoint)']) > 0 else np.nan,
            asca_results['explained_var']['C(timepoint)'][1] * 100 if len(asca_results['explained_var']['C(timepoint)']) > 1 else np.nan,
            asca_results['explained_var']['C(treatment):C(timepoint)'][0] * 100 if len(asca_results['explained_var']['C(treatment):C(timepoint)']) > 0 else np.nan,
            asca_results['explained_var']['C(treatment):C(timepoint)'][1] * 100 if len(asca_results['explained_var']['C(treatment):C(timepoint)']) > 1 else np.nan,
            np.nan, np.nan, np.nan, np.nan
        ],
        'PE-ASCA_pct': [
            pe_asca_results['explained_var'].get('C(treatment)', [0, 0])[0] * 100 if 'C(treatment)' in pe_asca_results['explained_var'] and len(pe_asca_results['explained_var']['C(treatment)']) > 0 else np.nan,
            pe_asca_results['explained_var'].get('C(treatment)', [0, 0])[1] * 100 if 'C(treatment)' in pe_asca_results['explained_var'] and len(pe_asca_results['explained_var']['C(treatment)']) > 1 else np.nan,
            pe_asca_results['explained_var'].get('C(timepoint)', [0, 0])[0] * 100 if 'C(timepoint)' in pe_asca_results['explained_var'] and len(pe_asca_results['explained_var']['C(timepoint)']) > 0 else np.nan,
            pe_asca_results['explained_var'].get('C(timepoint)', [0, 0])[1] * 100 if 'C(timepoint)' in pe_asca_results['explained_var'] and len(pe_asca_results['explained_var']['C(timepoint)']) > 1 else np.nan,
            pe_asca_results['explained_var'].get('C(treatment):C(timepoint)', [0, 0])[0] * 100 if 'C(treatment):C(timepoint)' in pe_asca_results['explained_var'] and len(pe_asca_results['explained_var']['C(treatment):C(timepoint)']) > 0 else np.nan,
            pe_asca_results['explained_var'].get('C(treatment):C(timepoint)', [0, 0])[1] * 100 if 'C(treatment):C(timepoint)' in pe_asca_results['explained_var'] and len(pe_asca_results['explained_var']['C(treatment):C(timepoint)']) > 1 else np.nan,
            np.nan, np.nan, np.nan, np.nan
        ],
        'MSCA_pct': [
            np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,
            msca_results['pca_between'].explained_variance_ratio_[0] * 100,
            msca_results['pca_between'].explained_variance_ratio_[1] * 100,
            msca_results['pca_within'].explained_variance_ratio_[0] * 100,
            msca_results['pca_within'].explained_variance_ratio_[1] * 100
        ]
    })

    # Export to CSV
    variance_summary.to_csv('variance_decomposition_summary.csv', index=False)
    pca_summary.to_csv('pca_explained_variance_summary.csv', index=False)

    print("Results exported to CSV files:")
    print("- variance_decomposition_summary.csv")
    print("- pca_explained_variance_summary.csv")

# Export results (if all methods completed successfully)
try:
    export_results_summary(df, asca_results, pe_asca_results, msca_results)
except Exception as e:
    print(f"Could not export all results: {e}")
    print("Make sure you've successfully run all methods before exporting.")

def asca_plus(data, formula, design_cols, feature_cols, n_permutations=999):
    """
    Implement ASCA+ (ASCA with permutation testing)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    formula: str
        ANOVA formula for decomposition
    design_cols: list
        List of column names for design factors
    feature_cols: list
        List of column names for features
    n_permutations: int
        Number of permutations for significance testing

    Returns:
    --------
    Dictionary containing ASCA results plus permutation test results
    """
    print("\n=== ASCA+ Analysis (ASCA with Permutation Testing) ===")

    # Run standard ASCA
    asca_results = asca(data, formula, design_cols, feature_cols)

    # Get effect matrices and calculate sum of squares for each effect
    effect_matrices = asca_results['effect_matrices']
    observed_ss = {effect: np.sum(matrix**2) for effect, matrix in effect_matrices.items()
                 if effect != 'Residual'}

    # Permutation testing
    permuted_ss = {effect: [] for effect in observed_ss.keys()}

    print(f"\nRunning permutation tests with {n_permutations} permutations...")

    for perm in range(n_permutations):
        if perm % 100 == 0 and perm > 0:
            print(f"  Completed {perm} permutations")

        # Create permuted data
        perm_data = data.copy()

        # Permute each factor separately
        for effect in observed_ss.keys():
            if effect == 'C(treatment)':
                # Permute treatment labels
                perm_data['treatment'] = np.random.permutation(perm_data['treatment'].values)
            elif effect == 'C(timepoint)':
                # Permute timepoint labels
                perm_data['timepoint'] = np.random.permutation(perm_data['timepoint'].values)
            elif effect == 'C(treatment):C(timepoint)':
                # Permute interaction by shuffling combinations
                combinations = perm_data[['treatment', 'timepoint']].values
                np.random.shuffle(combinations)
                perm_data['treatment'] = combinations[:, 0]
                perm_data['timepoint'] = combinations[:, 1]

        # Run ASCA on permuted data
        perm_asca = asca(perm_data, formula, design_cols, feature_cols)
        perm_effect_matrices = perm_asca['effect_matrices']

        # Calculate sum of squares for each permuted effect
        for effect in observed_ss.keys():
            perm_ss = np.sum(perm_effect_matrices[effect]**2)
            permuted_ss[effect].append(perm_ss)

    # Calculate p-values
    p_values = {}
    for effect in observed_ss.keys():
        # Count how many permuted SS are >= observed SS
        p_values[effect] = (np.sum(np.array(permuted_ss[effect]) >= observed_ss[effect]) + 1) / (n_permutations + 1)

    print("\nPermutation Test Results:")
    for effect, p_value in p_values.items():
        significance = "significant" if p_value < 0.05 else "not significant"
        print(f"{effect}: p-value = {p_value:.4f} ({significance})")

    # Add results to ASCA results
    asca_results['p_values'] = p_values
    asca_results['permuted_ss'] = permuted_ss

    return asca_results

def asca_plus(data, formula, design_cols, feature_cols, n_permutations=999):
    """
    Implement ASCA+ (ASCA with permutation testing)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    formula: str
        ANOVA formula for decomposition
    design_cols: list
        List of column names for design factors
    feature_cols: list
        List of column names for features
    n_permutations: int
        Number of permutations for significance testing

    Returns:
    --------
    Dictionary containing ASCA results plus permutation test results
    """
    print("\n=== ASCA+ Analysis (ASCA with Permutation Testing) ===")

    # Run standard ASCA
    asca_results = asca(data, formula, design_cols, feature_cols)

    # Get effect matrices and calculate sum of squares for each effect
    effect_matrices = asca_results['effect_matrices']
    observed_ss = {effect: np.sum(matrix**2) for effect, matrix in effect_matrices.items()
                 if effect != 'Residual'}

    # Permutation testing
    permuted_ss = {effect: [] for effect in observed_ss.keys()}

    print(f"\nRunning permutation tests with {n_permutations} permutations...")

    for perm in range(n_permutations):
        if perm % 100 == 0 and perm > 0:
            print(f"  Completed {perm} permutations")

        # Create permuted data
        perm_data = data.copy()

        # Permute each factor separately
        for effect in observed_ss.keys():
            if effect == 'C(treatment)':
                # Permute treatment labels
                perm_data['treatment'] = np.random.permutation(perm_data['treatment'].values)
            elif effect == 'C(timepoint)':
                # Permute timepoint labels
                perm_data['timepoint'] = np.random.permutation(perm_data['timepoint'].values)
            elif effect == 'C(treatment):C(timepoint)':
                # Permute interaction by shuffling combinations
                combinations = perm_data[['treatment', 'timepoint']].values
                np.random.shuffle(combinations)
                perm_data['treatment'] = combinations[:, 0]
                perm_data['timepoint'] = combinations[:, 1]

        # Run ASCA on permuted data
        perm_asca = asca(perm_data, formula, design_cols, feature_cols)
        perm_effect_matrices = perm_asca['effect_matrices']

        # Calculate sum of squares for each permuted effect
        for effect in observed_ss.keys():
            perm_ss = np.sum(perm_effect_matrices[effect]**2)
            permuted_ss[effect].append(perm_ss)

    # Calculate p-values
    p_values = {}
    for effect in observed_ss.keys():
        # Count how many permuted SS are >= observed SS
        p_values[effect] = (np.sum(np.array(permuted_ss[effect]) >= observed_ss[effect]) + 1) / (n_permutations + 1)

    print("\nPermutation Test Results:")
    for effect, p_value in p_values.items():
        significance = "significant" if p_value < 0.05 else "not significant"
        print(f"{effect}: p-value = {p_value:.4f} ({significance})")

    # Add results to ASCA results
    asca_results['p_values'] = p_values
    asca_results['permuted_ss'] = permuted_ss

    return asca_results

def pe_asca(data, formula, design_cols, feature_cols, n_permutations=999, alpha=0.05):
    """
    Implement PE-ASCA (Permutation-Enhanced ASCA)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    formula: str
        ANOVA formula for decomposition
    design_cols: list
        List of column names for design factors
    feature_cols: list
        List of column names for features
    n_permutations: int
        Number of permutations for significance testing
    alpha: float
        Significance level

    Returns:
    --------
    Dictionary containing PE-ASCA results
    """
    print("\n=== PE-ASCA Analysis (Permutation-Enhanced ASCA) ===")

    # First, run ASCA+ to get permutation test results
    asca_plus_results = asca_plus(data, formula, design_cols, feature_cols, n_permutations)

    # Extract relevant information
    effect_matrices = asca_plus_results['effect_matrices']
    p_values = asca_plus_results['p_values']

    # Enhanced decomposition: only keep significant effects
    enhanced_matrices = {}
    for effect, p_value in p_values.items():
        if p_value <= alpha:
            enhanced_matrices[effect] = effect_matrices[effect]
            print(f"Effect {effect} is significant (p={p_value:.4f}) and included in enhanced model")
        else:
            # Add non-significant effects to residuals
            if 'Residual' not in enhanced_matrices:
                enhanced_matrices['Residual'] = effect_matrices[effect].copy()
            else:
                enhanced_matrices['Residual'] += effect_matrices[effect]
            print(f"Effect {effect} is not significant (p={p_value:.4f}) and added to residuals")

    # Add original residuals if not already included
    if 'Residual' not in enhanced_matrices:
        enhanced_matrices['Residual'] = effect_matrices['Residual']
    else:
        enhanced_matrices['Residual'] += effect_matrices['Residual']

    # Apply PCA to each significant effect matrix
    pca_results = {}
    explained_var = {}

    for effect, matrix in enhanced_matrices.items():
        if effect != 'Residual':  # Skip PCA on residuals
            pca = PCA()
            pca_results[effect] = pca.fit_transform(matrix)
            explained_var[effect] = pca.explained_variance_ratio_

            # Print effect summary
            effect_ss = np.sum(matrix**2)
            total_ss = np.sum([np.sum(m**2) for m in enhanced_matrices.values()])
            print(f"{effect} - Sum of Squares: {effect_ss:.4f} ({effect_ss/total_ss*100:.2f}% of total)")
            if len(explained_var[effect]) > 0:
                print(f"  PC1 explained variance: {explained_var[effect][0]*100:.2f}%")
                if len(explained_var[effect]) > 1:
                    print(f"  PC2 explained variance: {explained_var[effect][1]*100:.2f}%")

    # Create result structure
    pe_asca_results = {
        'effect_matrices': enhanced_matrices,
        'pca_results': pca_results,
        'explained_var': explained_var,
        'p_values': p_values,
        'total_variance': asca_plus_results['total_variance']
    }

    return pe_asca_results

def limm_pca(data, feature_cols, verbose=True):
    """
    Implement LiMM-PCA (Linear Mixed Models with PCA)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    feature_cols: list
        List of column names for features
    verbose: bool
        Whether to print detailed output

    Returns:
    --------
    Dictionary containing LiMM-PCA results
    """
    if verbose:
        print("\n=== LiMM-PCA Analysis (Linear Mixed Models with PCA) ===")

    # Extract data matrix
    X = data[feature_cols].values

    # Standardize the data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Create copy of data with proper string conversion for categorical variables
    data_copy = data.copy()

    # Convert categorical variables to strings
    data_copy['subject_id'] = data_copy['subject_id'].astype(str)
    data_copy['treatment'] = data_copy['treatment'].astype(str)
    data_copy['timepoint'] = data_copy['timepoint'].astype(str)

    # Initialize containers for results
    fixed_effects = {}
    random_effects = {}
    residuals = {}
    pvalues = {}

    # Apply mixed model to each feature
    for i, feature in enumerate(feature_cols):
        if verbose and i % 3 == 0:
            print(f"Processing feature {i+1}/{len(feature_cols)}...")

        # Create mixed model formula
        formula = f"{feature} ~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"

        # Fit mixed effects model
        try:
            # Fit the model with explicit specification of groups
            model = mixedlm(formula, data_copy, groups=data_copy['subject_id'])
            result = model.fit(reml=True)

            # Extract fixed effects (predicted values without random effects)
            subjects = data_copy['subject_id'].values
            re_dict = {str(subj): val[0] for subj, val in result.random_effects.items()}

            # Create array for random effects
            subject_effect = np.array([re_dict.get(str(subj), 0) for subj in subjects])

            # Fixed effects = fitted values - random effects
            fixed_effects[feature] = result.fittedvalues - subject_effect

            # Store random effects
            random_effects[feature] = subject_effect

            # Extract residuals
            residuals[feature] = result.resid

            # Store p-values for fixed effects
            pvalues[feature] = result.pvalues

        except Exception as e:
            if verbose:
                print(f"Error fitting model for {feature}: {e}")
            # Use simpler approach if model fails
            fixed_effects[feature] = np.zeros(len(data))
            random_effects[feature] = np.zeros(len(data))
            residuals[feature] = X_scaled[:, i]

    # Convert to matrices
    fixed_matrix = np.column_stack([fixed_effects[feature] for feature in feature_cols])
    random_matrix = np.column_stack([random_effects[feature] for feature in feature_cols])
    resid_matrix = np.column_stack([residuals[feature] for feature in feature_cols])

    # Calculate variance explained
    total_var = np.sum(X_scaled**2)
    fixed_var = np.sum(fixed_matrix**2)
    random_var = np.sum(random_matrix**2)
    resid_var = np.sum(resid_matrix**2)

    if verbose:
        print("\nVariance Partition:")
        print(f"Fixed Effects: {fixed_var/total_var*100:.2f}%")
        print(f"Random Effects: {random_var/total_var*100:.2f}%")
        print(f"Residuals: {resid_var/total_var*100:.2f}%")

    # Apply PCA to fixed effects matrix
    pca_fixed = PCA()
    pca_fixed.fit(fixed_matrix)
    fixed_scores = pca_fixed.transform(fixed_matrix)

    if verbose and len(pca_fixed.explained_variance_ratio_) > 0:
        print("\nPCA on Fixed Effects:")
        print(f"PC1 explained variance: {pca_fixed.explained_variance_ratio_[0]*100:.2f}%")
        if len(pca_fixed.explained_variance_ratio_) > 1:
            print(f"PC2 explained variance: {pca_fixed.explained_variance_ratio_[1]*100:.2f}%")

    # Store results
    results = {
        'fixed_matrix': fixed_matrix,
        'random_matrix': random_matrix,
        'resid_matrix': resid_matrix,
        'pca_fixed': pca_fixed,
        'fixed_scores': fixed_scores,
        'total_var': total_var,
        'fixed_var': fixed_var,
        'random_var': random_var,
        'resid_var': resid_var
    }

    return results

def msca(data, feature_cols, verbose=True):
    """
    Implement MSCA (Multilevel Simultaneous Component Analysis)

    Parameters:
    -----------
    data: DataFrame
        Data containing design factors and feature measurements
    feature_cols: list
        List of column names for features
    verbose: bool
        Whether to print detailed output

    Returns:
    --------
    Dictionary containing MSCA results
    """
    if verbose:
        print("\n=== MSCA Analysis (Multilevel Simultaneous Component Analysis) ===")

    # Extract data
    X = data[feature_cols].values
    subjects = data['subject_id'].unique()
    n_subjects = len(subjects)

    # Center the data
    X_centered = X - np.mean(X, axis=0)

    # Split data into between-subject and within-subject parts
    X_between = np.zeros_like(X)
    X_within = np.zeros_like(X)

    # Calculate between-subject variation (level 2)
    for subject in subjects:
        subject_indices = data['subject_id'] == subject
        subject_mean = np.mean(X[subject_indices], axis=0)
        X_between[subject_indices] = subject_mean

    # Calculate within-subject variation (level 1)
    X_within = X - X_between

    # Calculate variance explained by each level
    total_var = np.sum(X_centered**2)
    between_var = np.sum(X_between**2)
    within_var = np.sum(X_within**2)

    if verbose:
        print("\nVariance Partition:")
        print(f"Between-subject: {between_var/total_var*100:.2f}%")
        print(f"Within-subject: {within_var/total_var*100:.2f}%")

    # Apply PCA to between-subject variation
    pca_between = PCA()
    between_scores = pca_between.fit_transform(X_between)

    # Apply PCA to within-subject variation
    pca_within = PCA()
    within_scores = pca_within.fit_transform(X_within)

    if verbose:
        print("\nPCA on Between-subject Variation:")
        print(f"PC1 explained variance: {pca_between.explained_variance_ratio_[0]*100:.2f}%")
        if len(pca_between.explained_variance_ratio_) > 1:
            print(f"PC2 explained variance: {pca_between.explained_variance_ratio_[1]*100:.2f}%")

        print("\nPCA on Within-subject Variation:")
        print(f"PC1 explained variance: {pca_within.explained_variance_ratio_[0]*100:.2f}%")
        if len(pca_within.explained_variance_ratio_) > 1:
            print(f"PC2 explained variance: {pca_within.explained_variance_ratio_[1]*100:.2f}%")

    # Store results
    results = {
        'X_between': X_between,
        'X_within': X_within,
        'pca_between': pca_between,
        'pca_within': pca_within,
        'between_scores': between_scores,
        'within_scores': within_scores,
        'total_var': total_var,
        'between_var': between_var,
        'within_var': within_var
    }

    return results

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
from statsmodels.formula.api import ols, mixedlm

# Prepare your data
# Either load from CSV:
# df = pd.read_csv('your_data.csv')
# Or use synthetic data generation function

# Extract feature columns and prepare data matrix
feature_cols = [col for col in df.columns if 'feature' in col]
X = df[feature_cols].values

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create a data matrix for easier handling
X_matrix = pd.DataFrame(X_scaled, columns=feature_cols)
X_matrix['subject_id'] = df['subject_id']
X_matrix['treatment'] = df['treatment']
X_matrix['timepoint'] = df['timepoint']

# Set up formula and design columns
formula = "~ C(treatment) + C(timepoint) + C(treatment):C(timepoint)"
design_cols = ['treatment', 'timepoint', 'subject_id']

# Run the analyses
asca_results = asca(X_matrix, formula, design_cols, feature_cols)
asca_plus_results = asca_plus(X_matrix, formula, design_cols, feature_cols, n_permutations=99)  # Use 999+ for final analysis
pe_asca_results = pe_asca(X_matrix, formula, design_cols, feature_cols, n_permutations=99, alpha=0.05)
limm_pca_results = limm_pca(X_matrix, feature_cols)
msca_results = msca(X_matrix, feature_cols)

# Now you can visualize and interpret the results
# (See the comprehensive plotting code in the guide)